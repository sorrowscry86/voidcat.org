<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Forbidden Library — VoidCat RDC</title>
  <meta name="description" content="Privacy‑first desktop LLM application with secure local storage and on‑device inference." />

  <!-- Open Graph Meta Tags -->
  <meta property="og:title" content="Forbidden Library — VoidCat RDC" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://voidcat.org/products/forbidden-library.html" />
  <meta property="og:image" content="https://voidcat.org/assets/logo-optimized.webp" />
  <meta property="og:description" content="Security-first, MCP-native agentic AI systems. Deployed to the edge." />
  <meta property="og:site_name" content="VoidCat RDC" />

  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Forbidden Library — VoidCat RDC" />
  <meta name="twitter:description" content="Security-first, MCP-native agentic AI systems. Deployed to the edge." />
  <meta name="twitter:image" content="https://voidcat.org/assets/logo-optimized.webp" />

  <!-- Canonical URL -->
  <link rel="canonical" href="https://voidcat.org/products/forbidden-library.html" />

  <link rel="icon" type="image/png" href="/assets/logo.png" />
  <link rel="stylesheet" href="/styles.css" />

  <!-- Web Analytics -->
  <script defer data-domain="voidcat.org" src="https://plausible.io/js/script.js"></script>
</head>
<body>
  <header class="site-header">
    <div class="container">
      <a class="brand" href="/">
        <img src="/assets/logo.png" alt="VoidCat RDC logo" width="40" height="40" />
        <span>VoidCat RDC</span>
      </a>
      <nav class="nav" aria-label="Primary">
        <a href="/products/" aria-current="page">Products</a>
        <a href="/solutions/">Solutions</a>
        <a href="/research/">Research</a>
        <a href="/podcasts/">Podcasts</a>
        <a href="/roadmap/">Roadmap</a>
        <a href="/company/">Company</a>
        <a href="/investors/">Investors</a>
        <a href="/projects/">Projects</a>
        <a href="/contact/">Contact</a>
      </nav>
    </div>
  </header>
  <main>
    <section class="hero"><div class="container">
      <h1>Forbidden Library</h1>
      <p>Privacy-first desktop AI application with local inference, secure storage, and zero data transmission. Control your AI experience.</p>
      <a class="cta" href="/contact/">Request Early Access</a>
    </div></section>

    <section class="section"><div class="container">
      <h2>The Problem</h2>
      <p>Cloud AI services capture sensitive data: trade secrets, personal information, confidential research. Enterprises in regulated industries (healthcare, finance, legal, government) cannot rely on external AI due to compliance requirements. Open-source models on personal hardware are powerful but fragmented—no unified workspace, poor performance, lack of professional tooling.</p>
      <p><strong>Market context:</strong> Privacy-sensitive teams and SMBs are a key ICP. Enterprise adoption of local AI growing from 5% (2024) to 25%+ (2026). Data sovereignty and regulatory compliance driving on-device inference adoption.</p>
    </div></section>

    <section class="section"><div class="container">
      <h2>The Solution</h2>
      <p>Forbidden Library combines powerful local inference with professional workspace management. Run state-of-the-art open models (Llama 2/3, Mistral, etc.) on your desktop or datacenter. Complete control. Zero data leakage. Enterprise-grade reliability.</p>
      <div class="grid">
        <article class="card"><h3>Local Inference</h3>
          <ul style="margin: 12px 0;">
            <li>Native Ollama integration</li>
            <li>Support for Llama, Mistral, Phi, more</li>
            <li>GPU acceleration (NVIDIA, Metal)</li>
            <li>Fallback to CPU with performance tuning</li>
          </ul>
        </article>
        <article class="card"><h3>Secure Workspace</h3>
          <ul style="margin: 12px 0;">
            <li>Encrypted local storage</li>
            <li>Multi-user profiles with isolation</li>
            <li>Session management &amp; audit logs</li>
            <li>Zero data transmission to cloud</li>
          </ul>
        </article>
        <article class="card"><h3>Professional Tools</h3>
          <ul style="margin: 12px 0;">
            <li>MCP tool integration</li>
            <li>Document management &amp; search</li>
            <li>Custom workflows &amp; automation</li>
            <li>Export to multiple formats</li>
          </ul>
        </article>
      </div>
    </section>

    <section class="section" style="background: linear-gradient(135deg, rgba(88, 86, 214, 0.05) 0%, rgba(6, 182, 212, 0.05) 100%);">
      <div class="container">
        <h2>How It Works</h2>
        <p style="color: var(--muted); margin-bottom: 32px;">Technical system flow: Privacy-first desktop AI from model installation to secure inference with zero data transmission.</p>
        
        <div style="display: grid; gap: 24px;">
          <article style="background: var(--panel); padding: 24px; border-radius: 8px; border-left: 4px solid var(--accent);">
            <h3 style="margin-top: 0;">Stage 1: Model Installation & Optimization</h3>
            <p><strong>User Action:</strong> Select model from catalog (Llama 3, Mistral, Phi, CodeLlama, etc.) via one-click interface.</p>
            <p><strong>Process:</strong> Model download from Ollama registry or Hugging Face (automatic source selection for best availability), local caching in content-addressable storage (deduplication of shared layers across models), GPU detection and driver validation (CUDA for NVIDIA, Metal for Apple Silicon, Vulkan for cross-platform), automatic quantization selection (Q4_K_M for 8GB RAM systems, Q5_K_M for 16GB+, FP16 for high-memory systems), performance benchmarking on first run (measure tokens/second for model + hardware combination).</p>
            <p><strong>Output:</strong> Optimized model ready for inference, stored locally with encryption at rest (AES-256).</p>
            <p style="margin-bottom: 0;"><strong>Technical Stack:</strong> Ollama integration, llama.cpp backend, Rust file system abstraction (Tauri), encrypted SQLite metadata store.</p>
          </article>

          <article style="background: var(--panel); padding: 24px; border-radius: 8px; border-left: 4px solid var(--accent);">
            <h3 style="margin-top: 0;">Stage 2: Workspace & Session Management</h3>
            <p><strong>User Action:</strong> Create project workspace or open existing conversation.</p>
            <p><strong>Process:</strong> Workspace isolation: each project stored in encrypted database partition (prevents cross-contamination), session initialization: load conversation history from local SQLite, context window management: automatic pruning of old messages to fit model context limits (typically 4K-32K tokens), tagging and search indexing: conversations indexed via full-text search for rapid retrieval, multi-user profile support: separate encrypted vaults per user (enterprise deployments).</p>
            <p><strong>Output:</strong> Active workspace with conversation history, tags, and metadata loaded into UI.</p>
            <p style="margin-bottom: 0;"><strong>Technical Stack:</strong> SQLite with FTS5 (full-text search), encryption via sqlcipher, SvelteKit frontend with reactive state management.</p>
          </article>

          <article style="background: var(--panel); padding: 24px; border-radius: 8px; border-left: 4px solid var(--accent);">
            <h3 style="margin-top: 0;">Stage 3: Local Inference Execution</h3>
            <p><strong>User Action:</strong> Submit prompt to model via chat interface.</p>
            <p><strong>Process:</strong> Prompt preprocessing: template application (system prompts, formatting), context assembly: inject conversation history + workspace documents if relevant, local inference via Ollama API: prompt sent to local llama.cpp server (no network transmission), GPU acceleration: CUDA kernels for NVIDIA, Metal Performance Shaders for Apple, CPU fallback with SIMD optimizations, streaming response: tokens generated incrementally, displayed in real-time, token counting and performance tracking: measure latency, tokens/second, memory usage.</p>
            <p><strong>Output:</strong> Model response streamed to UI, stored in encrypted local database, zero data leaves device.</p>
            <p style="margin-bottom: 0;"><strong>Technical Stack:</strong> Ollama REST API, llama.cpp inference engine, WebSocket streaming for UI updates, Rust backend for IPC (inter-process communication).</p>
          </article>

          <article style="background: var(--panel); padding: 24px; border-radius: 8px; border-left: 4px solid var(--accent);">
            <h3 style="margin-top: 0;">Stage 4: Tool Integration via MCP Protocol</h3>
            <p><strong>User Action:</strong> Enable MCP tools (code editor integration, database connectors, API clients).</p>
            <p><strong>Process:</strong> MCP server registration: external tools registered with permissions model (read/write access defined), tool invocation from model: LLM generates structured tool calls (JSON schema validated), local execution: tool runs on device with sandboxing (no external network unless explicitly authorized), result injection: tool output returned to model for next reasoning step, audit logging: all tool invocations logged with timestamps, parameters, results.</p>
            <p><strong>Output:</strong> Enhanced AI workflows with access to local files, databases, APIs—all executed on-device.</p>
            <p style="margin-bottom: 0;"><strong>Technical Stack:</strong> MCP protocol implementation (Rust), JSON schema validation, sandboxed subprocess execution, audit logs in encrypted SQLite.</p>
          </article>

          <article style="background: var(--panel); padding: 24px; border-radius: 8px; border-left: 4px solid var(--accent);">
            <h3 style="margin-top: 0;">Stage 5: Export & Sync (Optional)</h3>
            <p><strong>User Action:</strong> Export conversation/project or enable cross-device sync.</p>
            <p><strong>Process:</strong> Export: generate PDF/Word/Markdown from conversation with formatting preserved, encryption: if sync enabled, end-to-end encryption via CRDT (Conflict-free Replicated Data Types), sync mechanism: changes propagated to self-hosted sync server or VoidCat-managed option (user choice), decryption: only on authorized devices with user's encryption key (zero-knowledge architecture), conflict resolution: CRDTs ensure eventual consistency without data loss.</p>
            <p><strong>Output:</strong> Exported artifacts or synchronized workspace across devices, privacy maintained throughout.</p>
            <p style="margin-bottom: 0;"><strong>Technical Stack:</strong> Yrs CRDT library, ChaCha20-Poly1305 encryption, optional sync via WebSocket, export via pandoc integration.</p>
          </article>
        </div>

        <div style="margin-top: 32px; padding: 20px; background: var(--panel); border-radius: 8px; border: 1px solid rgba(88, 86, 214, 0.3);">
          <h4 style="margin-top: 0; color: var(--accent);">🔒 Privacy Architecture Guarantees</h4>
          <p style="margin-bottom: 0;"><strong>Zero Cloud Dependency:</strong> All inference runs locally via Ollama/llama.cpp—no API calls to external services.<br/>
          <strong>Encrypted Storage:</strong> All data encrypted at rest (AES-256) and in transit (if sync enabled, E2E encrypted).<br/>
          <strong>No Telemetry:</strong> Zero usage tracking, crash reporting optional and fully anonymized (user consent required).<br/>
          <strong>Audit Trail:</strong> Complete logs of all operations (model invocations, tool usage, data access) stored locally for compliance review.<br/>
          <strong>Open Source Core:</strong> Inference engine (llama.cpp) and runtime (Tauri) are open source and auditable.</p>
        </div>
      </div>
    </section>

    <section class="section"><div class="container">
      <h2>Core Features</h2>
      <ul style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 16px; list-style: none; padding: 0;">
        <li style="background: var(--panel); padding: 16px; border-radius: 8px;">
          <strong>Model Selection</strong><br/>One-click install of 50+ open models; automatic downloading, caching, optimization.
        </li>
        <li style="background: var(--panel); padding: 16px; border-radius: 8px;">
          <strong>Performance Tuning</strong><br/>GPU acceleration, batching, quantization; automatic fallback for hardware constraints.
        </li>
        <li style="background: var(--panel); padding: 16px; border-radius: 8px;">
          <strong>Workspace Management</strong><br/>Projects, conversations, tags, search; organize and retrieve work across sessions.
        </li>
        <li style="background: var(--panel); padding: 16px; border-radius: 8px;">
          <strong>Security &amp; Privacy</strong><br/>Encrypted storage, local-only execution, audit trails, compliance-ready.
        </li>
        <li style="background: var(--panel); padding: 16px; border-radius: 8px;">
          <strong>Tool Integration</strong><br/>MCP protocol support; connect to code editors, APIs, databases, external services.
        </li>
        <li style="background: var(--panel); padding: 16px; border-radius: 8px;">
          <strong>Cross-Platform</strong><br/>Mac, Windows, Linux; native performance; unified experience across desktop OS.
        </li>
      </ul>
    </div></section>

    <section class="section"><div class="container">
      <h2>Technical Architecture</h2>
      <table style="width: 100%; border-collapse: collapse; margin-top: 16px;">
        <thead>
          <tr style="background: var(--panel); border-bottom: 2px solid #1b2230;">
            <th style="padding: 12px; text-align: left; border-right: 1px solid #1b2230;">Component</th>
            <th style="padding: 12px; text-align: left;">Technology</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom: 1px solid #1b2230;">
            <td style="padding: 12px; border-right: 1px solid #1b2230;"><strong>Desktop Runtime</strong></td>
            <td style="padding: 12px;">Tauri (Rust backend + webview frontend)</td>
          </tr>
          <tr style="border-bottom: 1px solid #1b2230;">
            <td style="padding: 12px; border-right: 1px solid #1b2230;"><strong>Frontend</strong></td>
            <td style="padding: 12px;">SvelteKit with TypeScript; responsive UI</td>
          </tr>
          <tr style="border-bottom: 1px solid #1b2230;">
            <td style="padding: 12px; border-right: 1px solid #1b2230;"><strong>Model Runtime</strong></td>
            <td style="padding: 12px;">Ollama + llama.cpp; GPU support (CUDA, Metal, Vulkan)</td>
          </tr>
          <tr style="border-bottom: 1px solid #1b2230;">
            <td style="padding: 12px; border-right: 1px solid #1b2230;"><strong>Storage</strong></td>
            <td style="padding: 12px;">SQLite (local), encrypted filesystem, content-addressable storage</td>
          </tr>
          <tr style="border-bottom: 1px solid #1b2230;">
            <td style="padding: 12px; border-right: 1px solid #1b2230;"><strong>Sync (Optional)</strong></td>
            <td style="padding: 12px;">End-to-end encrypted sync via CRDTs; self-hosted or managed option</td>
          </tr>
          <tr>
            <td style="padding: 12px; border-right: 1px solid #1b2230;"><strong>Compliance</strong></td>
            <td style="padding: 12px;">Audit logging, no telemetry, local-only processing, SOC2-ready</td>
          </tr>
        </tbody>
      </table>
    </div></section>

    <section class="section"><div class="container">
      <h2>Use Cases &amp; Customer Profiles</h2>
      <div class="grid">
        <article class="card"><h3>Regulated Industries</h3>
          <p>Healthcare, finance, legal professionals needing HIPAA/PCI/SOX compliance. Local inference = no data transmission.</p>
        </article>
        <article class="card"><h3>Researchers &amp; Scientists</h3>
          <p>Run models on proprietary datasets without uploading to cloud. Full reproducibility, version control.</p>
        </article>
        <article class="card"><h3>Individual Privacy Champions</h3>
          <p>Founders, consultants, creatives protecting intellectual property. No vendor lock-in, full control.</p>
        </article>
        <article class="card"><h3>Enterprise Security Teams</h3>
          <p>Internal deployments on secure networks. Airgapped systems, compliance auditing, centralized management.</p>
        </article>
      </div>
    </div></section>

    <section class="section"><div class="container">
      <h2>Supported Models</h2>
      <div class="grid">
        <article class="card"><h3>Open Foundation</h3>
          <ul style="margin: 12px 0; font-size: 14px;">
            <li>Llama 2/3 (7B, 13B, 70B variants)</li>
            <li>Mistral 7B</li>
            <li>Phi-2/3</li>
          </ul>
        </article>
        <article class="card"><h3>Specialized</h3>
          <ul style="margin: 12px 0; font-size: 14px;">
            <li>Code models (StarCoder, CodeLlama)</li>
            <li>Math models (MetaMath, Llemma)</li>
            <li>Domain-specific fine-tunes</li>
          </ul>
        </article>
        <article class="card"><h3>Performance Tiers</h3>
          <ul style="margin: 12px 0; font-size: 14px;">
            <li>Fast: 7B models (4GB+ RAM)</li>
            <li>Balanced: 13B models (8GB+ RAM)</li>
            <li>Powerful: 70B+ (24GB+ VRAM)</li>
          </ul>
        </article>
      </div>
    </div></section>

    <section class="section"><div class="container">
      <h2>Pricing &amp; Distribution</h2>
      <ul style="display: grid; gap: 12px;">
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--brand);">
          <strong>Free Tier:</strong> Personal use, open source, community support. Model library included.
        </li>
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--accent);">
          <strong>Pro ($49/year):</strong> Priority support, premium models, cloud sync, professional themes.
        </li>
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--brand);">
          <strong>Team/Enterprise (Custom):</strong> Site licensing, compliance packages, white-label, managed deployment.
        </li>
      </ul>
    </div></section>

    <section class="section"><div class="container">
      <h2>Key Differentiators</h2>
      <ul style="display: grid; gap: 12px;">
        <li style="background: var(--panel); padding: 12px;">
          <strong>Privacy by Design:</strong> Zero data transmission; local storage encrypted; audit-ready architecture.
        </li>
        <li style="background: var(--panel); padding: 12px;">
          <strong>Enterprise Ready:</strong> Professional workspace management, team collaboration, compliance support.
        </li>
        <li style="background: var(--panel); padding: 12px;">
          <strong>Performance Optimized:</strong> GPU acceleration automatic; quantization available; benchmark-leading latency.
        </li>
        <li style="background: var(--panel); padding: 12px;">
          <strong>Tool Ecosystem:</strong> MCP integration enables connection to IDEs, APIs, databases, external services.
        </li>
        <li style="background: var(--panel); padding: 12px;">
          <strong>Cross-Platform Native:</strong> Mac, Windows, Linux with native performance; not web-wrapped.
        </li>
        <li style="background: var(--panel); padding: 12px;">
          <strong>Model Flexibility:</strong> Swap models instantly; no vendor lock-in; use open source or custom models.
        </li>
      </ul>
    </div></section>

    <section class="section"><div class="container">
      <h2>Development Status &amp; Roadmap</h2>
      <ul style="display: grid; gap: 12px;">
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--brand);">
          <strong>Q1 2026:</strong> Private beta with 500 founders; Mac/Windows native apps; model marketplace beta.
        </li>
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--accent);">
          <strong>Q2 2026:</strong> Public beta; Linux support; team collaboration features; cloud sync (optional).
        </li>
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--brand);">
          <strong>Q3 2026:</strong> Enterprise tier launch; compliance certifications (SOC2, HIPAA); white-label option.
        </li>
        <li style="background: var(--panel); padding: 12px; border-left: 3px solid var(--accent);">
          <strong>Q4 2026:</strong> v1.0 production; ecosystem partnerships; mobile companion app.
        </li>
      </ul>
    </div></section>

    <section class="section"><div class="container">
      <h2>Get Started</h2>
      <p style="margin-bottom: 16px;">Forbidden Library is in private beta. Early access available for founders, researchers, and privacy-focused organizations.</p>
      <a class="cta" href="/contact/">Request Early Access</a>
    </div></section>
  </main>
  <footer class="site-footer"><div class="container"><p>© <span id="year"></span> VoidCat RDC, LLC.</p></div></footer>
  <script>document.getElementById('year').textContent = new Date().getFullYear();</script>
</body>
</html>
